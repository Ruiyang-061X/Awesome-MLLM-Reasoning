<a name="readme-top"></a>

<div align="center">
  <a href="https://github.com/Ruiyang-061X/Awesome-MLLM-Reasoning/stargazers"><img src="https://img.shields.io/github/stars/Ruiyang-061X/Awesome-MLLM-Reasoning?style=for-the-badge" alt="Stargazers"></a>
  <a href="https://github.com/Ruiyang-061X/Awesome-MLLM-Reasoning/network/members"><img src="https://img.shields.io/github/forks/Ruiyang-061X/Awesome-MLLM-Reasoning?style=for-the-badge" alt="Forks"></a>
  <a href="https://github.com/Ruiyang-061X/Awesome-MLLM-Reasoning/graphs/contributors"><img src="https://img.shields.io/github/contributors/Ruiyang-061X/Awesome-MLLM-Reasoning?style=for-the-badge" alt="Contributors"></a>
  <a href="https://github.com/Ruiyang-061X/Awesome-MLLM-Reasoning/blob/main/LICENSE"><img src="https://img.shields.io/github/license/Ruiyang-061X/Awesome-MLLM-Reasoning?style=for-the-badge" alt="MIT License"></a>
</div>

<h1 align="center">Awesome MLLM Reasoning</h1>

### :star::star::star: If you find this repo useful, please star it!

> üî• for papers with >50 citations or repositories with >200 stars.\
> üìñ for papers accepted by reputed conferences/journals.

## üìù Survey

### 2025

1. **[Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models](https://arxiv.org/abs/2501.09686)** (16 Jan 2025)

   *Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li*

### 2024

1. **[Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning](https://arxiv.org/abs/2401.06805)** (10 Jan 2024)

   *Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, Hongxia Yang*

1. **[Reasoning with Large Language Models, a Survey](https://arxiv.org/abs/2407.11511)** (16 Jul 2024)

   *Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back*

1. üìñ **[LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models](https://arxiv.org/abs/2404.01230)** (1 Apr 2024, COLM 2024)

   *Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei*

### 2022

1. üî•üìñ **[Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)** (20 Dec 2022, ACL 2023 Findings)

   *Jie Huang, Kevin Chen-Chuan Chang*

## üìà Benchmark

### 2025

1. **EMMA [Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark](https://arxiv.org/abs/2501.05444)** [[Code](https://github.com/hychaochao/EMMA)] (9 Jan 2025)

   *Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Yu Cheng*

### 2024

1. **Polymath [Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark](https://arxiv.org/abs/2410.14702)** [[Code](https://github.com/kevinscaria/PolyMATH)] (6 Oct 2024)

   *Himanshu Gupta, Shreyas Verma, Ujjwala Anantheswaran, Kevin Scaria, Mihir Parmar, Swaroop Mishra, Chitta Baral*

1. üìñ **MLLM-CompBench [MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs](https://arxiv.org/abs/2407.16837)** [[Code](https://github.com/RaptorMai/CompBench)] (23 Jul 2024, NeurIPS 2024)

   *Jihyung Kil, Zheda Mai, Justin Lee, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, Arpita Chowdhury, Wei-Lun Chao*

1. **LogicVista [LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts](https://arxiv.org/abs/2407.04973)** [[Code](https://github.com/Yijia-Xiao/LogicVista)] (6 Jul 2024)

   *Yijia Xiao, Edward Sun, Tianyu Liu, Wei Wang*

1. üî•üìñ **Visual CoT [Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning](https://arxiv.org/abs/2403.16999)** [[Code](https://github.com/deepcs233/Visual-CoT)] (25 Mar 2024, NeurIPS 2024 Spotlight)

   *Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li*

1. üî• **Mementos [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences](https://arxiv.org/abs/2401.10529)** [[Code](https://github.com/umd-huang-lab/Mementos)] (19 Jan 2024)

   *Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong Huang*

<p align="right" style="font-size: 14px; color: #555; margin-top: 20px;">
    <a href="#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;">
        ‚Üë Back to Top ‚Üë
    </a>
</p>

## ‚ú® Paper

### 2025

1. **Virgo [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904)** [[Code](https://github.com/RUCAIBox/Virgo)] (3 Jan 2025)

   *Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen*


   *Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng*

### 2024

1. üî• **Mulberry [Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/abs/2412.18319)** [[Code](https://github.com/HJYao00/Mulberry)] (24 Dec 2024)

   *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*

1. **AtomThink [AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2411.11930)** [[Code](https://github.com/Quinn777/AtomThink)] (18 Nov 2024)

   *Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, Lanqing Hong, Hang Xu, Xiaodan Liang*

1. üî• **LLaVA-CoT [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)** [[Code](https://github.com/PKU-YuanGroup/LLaVA-CoT)] (15 Nov 2024)

   *Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan*

1. üìñ **IRED [Learning Iterative Reasoning through Energy Diffusion](https://arxiv.org/abs/2406.11179)** [[Code](https://github.com/yilundu/ired_code_release)] [[Website](https://energy-based-model.github.io/ired/)] (17 Jun 2024, ICML 2024)

   *Yilun Du, Jiayuan Mao, Joshua B. Tenenbaum*

1. üìñ **GeoReasoner [GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model](https://arxiv.org/abs/2406.18572)** [[Code](https://github.com/lingli1996/GeoReasoner)] (3 Jun 2024, ICML 2024)

   *Ling Li, Yu Ye, Bingchuan Jiang, Wei Zeng*

1. üî•üìñ **VoT [Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition](https://arxiv.org/abs/2501.03230)** [[Code](https://github.com/scofield7419/Video-of-Thought)] [[Website](https://haofei.vip/VoT/)] (7 May 2024, ICML 2024 Oral)

   *Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, Wynne Hsu*

1. **Cantor [Cantor: Inspiring Multimodal Chain-of-Thought of MLLM](https://arxiv.org/abs/2404.16033)** [[Code](https://github.com/ggg0919/cantor)] (24 Apr 2024)

   *Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, Rongrong Ji*

1. üìñ **Momentor [Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning](https://arxiv.org/abs/2501.01904)** [[Code](https://github.com/DCDmllm/Momentor)] (18 Feb 2024, ICML 2024)

   *Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, Siliang Tang*

1. üìñ **ContPhy [ContPhy: Continuum Physical Concept Learning and Reasoning from Videos](https://arxiv.org/abs/2402.06119)** [[Code](https://github.com/Cakeyan/ContPhy_Public)] [[Website](https://physical-reasoning-project.github.io/)] (9 Feb 2024, ICML 2024)

   *Zhicheng Zheng, Xin Yan, Zhenfang Chen, Jingzhou Wang, Qin Zhi Eddie Lim, Joshua B. Tenenbaum, Chuang Gan*

1. üìñ **ConTextual [ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models](https://arxiv.org/abs/2401.13311)** [[Code](https://github.com/rohan598/ConTextual)] [[Website](https://con-textual.github.io/)] (24 Jan 2024, ICML 2024)

### 2023

1. üî•üìñ **MM-CoT [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)** [[Code](https://github.com/amazon-science/mm-cot)] (2 Feb 2023, TMLR)

   *Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola*

<p align="right" style="font-size: 14px; color: #555; margin-top: 20px;">
    <a href="#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;">
        ‚Üë Back to Top ‚Üë
    </a>
</p>

## üç∫ Contributing

All issues and pull requests are warmly welcomed to contribute related papers to this curated list! Feel free to submit any relevant additions to help expand and enhance this collection.

## üí° Contributors

<a href="https://github.com/Ruiyang-061X/Awesome-MLLM-Reasoning/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Ruiyang-061X/Awesome-MLLM-Reasoning" />
</a>