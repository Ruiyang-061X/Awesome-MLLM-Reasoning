# Awesome-MLLM-Reasoning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

### :star::star::star: If you find this repo useful, please star it!

## üìù Survey

+ [Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning](https://arxiv.org/abs/2401.06805) (10 Jan 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.06805)

+ [Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models](https://arxiv.org/abs/2501.09686) (16 Jan 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.09686)

+ [Reasoning with Large Language Models, a Survey](https://arxiv.org/abs/2407.11511) (16 Jul 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.11511)

+ [LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models](https://arxiv.org/abs/2404.01230) (1 Apr 2024, COLM 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.01230)

+ [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403) (20 Dec 2022, ACL 2023 Findings, 15 pages)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.10403)

## üìà Benchmark

+ **EMMA** [Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark](https://arxiv.org/abs/2501.05444) (9 Jan 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.05444)
  [![Star](https://img.shields.io/github/stars/hychaochao/EMMA.svg?style=social&label=Star)](https://github.com/hychaochao/EMMA)

+ **Polymath** [Polymath: A Challenging Multi-modal Mathematical Reasoning Benchmark](https://arxiv.org/abs/2410.14702) (6 Oct 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.14702)
  [![Star](https://img.shields.io/github/stars/kevinscaria/PolyMATH.svg?style=social&label=Star)](https://github.com/kevinscaria/PolyMATH)

+ **MLLM-CompBench** [MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs](https://arxiv.org/abs/2407.16837) (23 Jul 2024, NeurIPS 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.16837)
  [![Star](https://img.shields.io/github/stars/RaptorMai/CompBench.svg?style=social&label=Star)](https://github.com/RaptorMai/CompBench)

+ **LogicVista** [LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts](https://arxiv.org/abs/2407.04973) (6 Jul 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.04973)
  [![Star](https://img.shields.io/github/stars/Yijia-Xiao/LogicVista.svg?style=social&label=Star)](https://github.com/Yijia-Xiao/LogicVista)

+ **Visual CoT** [Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning](https://arxiv.org/abs/2403.16999) (25 Mar 2024, NeurIPS 2024 Spotlight)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.16999)
  [![Star](https://img.shields.io/github/stars/deepcs233/Visual-CoT.svg?style=social&label=Star)](https://github.com/deepcs233/Visual-CoT)

+ **Mementos** [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences](https://arxiv.org/abs/2401.10529) (19 Jan 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2401.10529)
  [![Star](https://img.shields.io/github/stars/umd-huang-lab/Mementos.svg?style=social&label=Star)](https://github.com/umd-huang-lab/Mementos)

## ‚ú® Paper

+ **Virgo** [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904) (3 Jan 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.01904)
  [![Star](https://img.shields.io/github/stars/RUCAIBox/Virgo.svg?style=social&label=Star)](https://github.com/RUCAIBox/Virgo)

+ **Mulberry** [Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/abs/2412.18319) (24 Dec 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.18319)
  [![Star](https://img.shields.io/github/stars/HJYao00/Mulberry.svg?style=social&label=Star)](https://github.com/HJYao00/Mulberry)

+ **AtomThink** [AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2411.11930) (18 Nov 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.11930)
  [![Star](https://img.shields.io/github/stars/Quinn777/AtomThink.svg?style=social&label=Star)](https://github.com/Quinn777/AtomThink)

+ **LLaVA-CoT** [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440) (15 Nov 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2411.10440)
  [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/LLaVA-CoT.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/LLaVA-CoT)

+ **Cantor** [Cantor: Inspiring Multimodal Chain-of-Thought of MLLM](https://arxiv.org/abs/2404.16033) (24 Apr 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.16033)
  [![Star](https://img.shields.io/github/stars/ggg0919/cantor.svg?style=social&label=Star)](https://github.com/ggg0919/cantor)

+ **MM-CoT** [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) (2 Feb 2023, TMLR)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.00923)
  [![Star](https://img.shields.io/github/stars/amazon-science/mm-cot.svg?style=social&label=Star)](https://github.com/amazon-science/mm-cot)
